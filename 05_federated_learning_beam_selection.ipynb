{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning\n",
    "\n",
    "## An example of Federated Learning applied to beam selection using LIDAR data - From ITU Challenge - https://github.com/ITU-AI-ML-in-5G-Challenge/PS-012-ML5G-PHY-Beam-Selection_Imperial_IPC1 -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import math\n",
    "from src.dataloader import LidarDataset2D\n",
    "from src.models import Lidar2D\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################Arguments################################\n",
    "MONTECARLO = 1\n",
    "NUM_VEHICLES = 2\n",
    "LOCAL_EPOCHS = 1\n",
    "AGGREGATION_ROUNDS = 30\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "SHUFFLE_BUFFER = 20\n",
    "PREFETCH_BUFFER=10\n",
    "\n",
    "#Remake train and test split data\n",
    "lidar_training_path = [\"lidar_input_train.npz\", \"lidar_input_validation.npz\"] ###Raymobtime s008\n",
    "beam_training_path = [\"beams_output_train.npz\", \"beams_output_validation.npz\"] ###Raymobtime s008\n",
    "\n",
    "lidar_test_path = [\"lidar_input_test.npz\"] ###Raymobtime s009\n",
    "beam_test_path = [\"beams_output_test.npz\"] ###Raymobtime s009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset process functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_dataset(lidar_path, beam_path, num_vehicles, vehicle_ID):\n",
    "    training_data = LidarDataset2D(lidar_path, beam_path)\n",
    "    training_data.lidar_data = np.transpose(training_data.lidar_data, (0, 2, 3, 1))\n",
    "    x=training_data.lidar_data\n",
    "    xx = x[vehicle_ID*int(x.shape[0]/num_vehicles):(vehicle_ID+1)*int(x.shape[0]/num_vehicles),:,:,:] ###Split Lidar Data\n",
    "    y = training_data.beam_output\n",
    "    yy = y[vehicle_ID*int(y.shape[0]/num_vehicles):(vehicle_ID+1)*int(y.shape[0]/num_vehicles),:] ###Split Beam Labels\n",
    "    \n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices((list(xx.astype(np.float32)),list(yy.astype(np.float32))))\n",
    "    #sio.savemat('label'+str(k)+'.mat',{'label'+str(k):yy})\n",
    "    return dataset_train\n",
    "\n",
    "def get_test_dataset(lidar_path, beam_path):\n",
    "    test_data = LidarDataset2D(lidar_path, beam_path)\n",
    "    test_data.lidar_data = np.transpose(test_data.lidar_data, (0, 2, 3, 1))\n",
    "    dataset_test = tf.data.Dataset.from_tensor_slices((list(test_data.lidar_data.astype(np.float32)),list(test_data.beam_output.astype(np.float32))))\n",
    "    return dataset_test\n",
    "\n",
    "def preprocess(dataset):\n",
    "  def batch_format_fn(element1,element2):\n",
    "    return collections.OrderedDict(x=element1, y=element2)\n",
    "  return dataset.repeat(LOCAL_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(20, 200, 1)),\n",
    "    tf.keras.layers.Conv2D(5, 3, 1, padding='same'),# kernel_initializer=initializers.HeUniform),\n",
    "    tf.keras.layers.BatchNormalization(axis=3),\n",
    "    tf.keras.layers.PReLU(shared_axes=[1, 2]),\n",
    "    tf.keras.layers.Conv2D(5, 3, 1, padding='same'),# kernel_initializer=initializers.HeUniform),\n",
    "    tf.keras.layers.BatchNormalization(axis=3),\n",
    "    tf.keras.layers.PReLU(shared_axes=[1, 2]),\n",
    "    tf.keras.layers.Conv2D(5, 3, 2, padding='same'),# kernel_initializer=initializers.HeUniform),\n",
    "    tf.keras.layers.BatchNormalization(axis=3),\n",
    "    tf.keras.layers.PReLU(shared_axes=[1, 2]),\n",
    "    tf.keras.layers.Conv2D(5, 3, 1, padding='same'),# kernel_initializer=initializers.HeUniform),\n",
    "    tf.keras.layers.BatchNormalization(axis=3),\n",
    "    tf.keras.layers.PReLU(shared_axes=[1, 2]),\n",
    "    tf.keras.layers.Conv2D(5, 3, 2, padding='same'),#, kernel_initializer=initializers.HeUniform),\n",
    "    tf.keras.layers.BatchNormalization(axis=3),\n",
    "    tf.keras.layers.PReLU(shared_axes=[1, 2]),\n",
    "    tf.keras.layers.Conv2D(1, 3, (1, 2), padding='same'),#, kernel_initializer=initializers.HeUniform),\n",
    "    tf.keras.layers.BatchNormalization(axis=3),\n",
    "    tf.keras.layers.PReLU(shared_axes=[1, 2]),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(16),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    # layers.Dropout(0.7),\n",
    "    tf.keras.layers.Dense(256),\n",
    "    tf.keras.layers.Softmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 10:06:44.190938: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 89552000 exceeds 10% of free system memory.\n",
      "2022-09-09 10:07:01.047212: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 89552000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "temp_dataset = get_local_dataset(lidar_training_path, beam_training_path,NUM_VEHICLES,0)\n",
    "preprocessed_example_dataset=preprocess(temp_dataset)\n",
    "example_element = next(iter((preprocessed_example_dataset)))\n",
    "\n",
    "def model_fn():\n",
    "  keras_model = create_keras_model()\n",
    "  top1 = tf.keras.metrics.TopKCategoricalAccuracy(k=1, name='top_1_categorical_accuracy', dtype=None)\n",
    "  top10 = tf.keras.metrics.TopKCategoricalAccuracy(k=10, name='top_10_categorical_accuracy', dtype=None)\n",
    "  return tff.learning.from_keras_model(keras_model,\n",
    "      input_spec=preprocessed_example_dataset.element_spec,\n",
    "      loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "      metrics=[top1,top10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takashi/anaconda3/envs/ai6g/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:371: UserWarning: Batch Normalization contains non-trainable variables that won't be updated during the training. Consider using Group Normalization instead.\n",
      "  return py_builtins.overload_of(f)(*args)\n",
      "/home/takashi/anaconda3/envs/ai6g/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loop\n",
      "OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('top_1_categorical_accuracy', 0.24298732), ('top_10_categorical_accuracy', 0.72279793), ('loss', 3.0880027), ('num_examples', 11194), ('num_batches', 5598)]))])\n",
      "OrderedDict([('top_1_categorical_accuracy', 0.12129072), ('top_10_categorical_accuracy', 0.26073873), ('loss', 5.347717), ('num_examples', 9638), ('num_batches', 4819)])\n",
      "OrderedDict([('top_1_categorical_accuracy', 0.12129072), ('top_10_categorical_accuracy', 0.26073873), ('loss', 5.347717), ('num_examples', 9638), ('num_batches', 4819)])\n",
      "OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('top_1_categorical_accuracy', 0.29408613), ('top_10_categorical_accuracy', 0.7822941), ('loss', 2.8385506), ('num_examples', 11194), ('num_batches', 5598)]))])\n",
      "OrderedDict([('top_1_categorical_accuracy', 0.13280764), ('top_10_categorical_accuracy', 0.68489313), ('loss', 4.5988417), ('num_examples', 9638), ('num_batches', 4819)])\n",
      "OrderedDict([('top_1_categorical_accuracy', 0.13280764), ('top_10_categorical_accuracy', 0.68489313), ('loss', 4.5988417), ('num_examples', 9638), ('num_batches', 4819)])\n",
      "0\n",
      "0.7822940945625305\n"
     ]
    }
   ],
   "source": [
    "iterative_process = tff.learning.build_federated_averaging_process(\n",
    "    model_fn,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(lr=5e-3),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(lr=.25))\n",
    "\n",
    "evaluation = tff.learning.build_federated_evaluation(model_fn)\n",
    "\n",
    "test_data = LidarDataset2D(lidar_test_path, beam_test_path)\n",
    "test_data.lidar_data = np.transpose(test_data.lidar_data, (0, 2, 3, 1))\n",
    "\n",
    "accFL=0  \n",
    "for MONTECARLOi in range(MONTECARLO):\n",
    "    ###Generate Federated Train Dataset\n",
    "    federated_train_data=[]     \n",
    "    for i in range(NUM_VEHICLES):\n",
    "        train_dataset = get_local_dataset(lidar_training_path, beam_training_path,NUM_VEHICLES,i)\n",
    "        federated_train_data.append(preprocess(train_dataset))\n",
    "        \n",
    "    ###Generate Test Dataset\n",
    "    test_dataset = get_test_dataset(lidar_test_path, beam_test_path)\n",
    "    federated_test_data=[preprocess(test_dataset)]\n",
    "    \n",
    "    top1=np.zeros(AGGREGATION_ROUNDS)\n",
    "    top10=np.zeros(AGGREGATION_ROUNDS)\n",
    "    \n",
    "    state = iterative_process.initialize() ###Initialize training\n",
    "    \n",
    "    #top1 = tf.keras.metrics.TopKCategoricalAccuracy(k=1, name='top_1_categorical_accuracy', dtype=None)\n",
    "    #top10 = tf.keras.metrics.TopKCategoricalAccuracy(k=10, name='top_10_categorical_accuracy', dtype=None)\n",
    "    \n",
    "    ###Federated Training    \n",
    "    for round_num in range(AGGREGATION_ROUNDS):\n",
    "      state, metrics = iterative_process.next(state, federated_train_data)\n",
    "      test_metrics = evaluation(state.model, federated_test_data)['eval']\n",
    "      \n",
    "      print(str(metrics))\n",
    "      print(str(test_metrics))\n",
    "      print(test_metrics)\n",
    "      \n",
    "      top1[round_num]=test_metrics['top_1_categorical_accuracy']\n",
    "      top10[round_num]=test_metrics['top_10_categorical_accuracy']\n",
    "    \n",
    "      ###Generate Accuracy and Throughput Performance Curves\n",
    "      keras_model = create_keras_model()\n",
    "      #keras_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[top1,top10])\n",
    "      state.model.assign_weights_to(keras_model)\n",
    "      test_preds = keras_model.predict(test_data.lidar_data, batch_size=100)\n",
    "      test_preds_idx = np.argsort(test_preds, axis=1)\n",
    "      top_k = np.zeros(100)\n",
    "      throughput_ratio_at_k = np.zeros(100)\n",
    "      correct = 0\n",
    "      for i in range(100):\n",
    "        correct += np.sum(test_preds_idx[:, -1-i] == np.argmax(test_data.beam_output, axis=1))\n",
    "        top_k[i] = correct/test_data.beam_output.shape[0]\n",
    "        throughput_ratio_at_k[i] = np.sum(np.log2(np.max(np.take_along_axis(test_data.beam_output_true, test_preds_idx, axis=1)[:, -1-i:], axis=1) + 1.0))/\\\n",
    "                                   np.sum(np.log2(np.max(test_data.beam_output_true, axis=1) + 1.0))\n",
    "        \n",
    "      sio.savemat('federated_accuracy'+str(round_num)+'.mat',{'accuracy':top_k})\n",
    "      sio.savemat('federated_throughput'+str(round_num)+'.mat',{'throughput':throughput_ratio_at_k})\n",
    "    \n",
    "    sio.savemat('top1.mat',{'top1':top1})\n",
    "    sio.savemat('top10.mat',{'top10':top10})\n",
    "    \n",
    "    np.savez(\"federated.npz\", classification=top_k, throughput_ratio=throughput_ratio_at_k)\n",
    "    accFL=accFL+metrics['train']['top_10_categorical_accuracy']/MONTECARLO\n",
    "    \n",
    "    print(MONTECARLOi)\n",
    "    \n",
    "print(accFL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ai6g')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1011682ff88325c274505c7bc567ab5116af6434e0de5fe7e03fb39522c23be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
